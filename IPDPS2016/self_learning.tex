
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[10pt, conference, compsocconf]{IEEEtran}
% Add the compsocconf option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%

\usepackage[cmex10]{amsmath}
\usepackage{tikz}
 
\definecolor{bluekeywords}{rgb}{0,0,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.64,0.08,0.08}
\definecolor{xmlcomments}{rgb}{0.5,0.5,0.5}
\definecolor{types}{rgb}{0.17,0.57,0.68}

\usepackage{listings}
\lstset{language=[Sharp]C,
captionpos=b,
numbers=left, %Nummerierung
numberstyle=\tiny, % kleine Zeilennummern
frame=lines, % Oberhalb und unterhalb des Listings ist eine Linie
showspaces=false,
showtabs=false,
breaklines=true,
showstringspaces=false,
breakatwhitespace=true,
escapeinside={(*@}{@*)},
commentstyle=\color{greencomments},
morekeywords={partial, var, value, get, set},
keywordstyle=\color{bluekeywords},
stringstyle=\color{redstrings},
basicstyle=\scriptsize,
}

\usepackage[]{algorithm}
\usepackage{algorithmic}

\usepackage{etoolbox}
\newcommand{\algorithmicdoinparallel}{\textbf{do in parallel}}
\makeatletter
\AtBeginEnvironment{algorithmic}{%
  \newcommand{\FORALLP}[2][default]{\ALC@it\algorithmicforall\ #2\ %
    \algorithmicdoinparallel\ALC@com{#1}\begin{ALC@for}}%
}
\makeatother


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{property}[theorem]{Property}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{An automatic tuning system for  NP-hard algorithms in a  cloud context}


% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

\author{\IEEEauthorblockN{Yanik Ngoko}
\IEEEauthorblockA{Qarnot Computing and LIPN, University of Paris 13\\
Paris, France\\
yanik.ngoko@qarnot-computing.com}
\and 
\IEEEauthorblockN{Denis Trystram, Valentin Reis}
\IEEEauthorblockA{LIG, Grenoble University\\
Grenoble, France\\
denis.trystram@imag.fr}
\and 
\IEEEauthorblockN{Christophe C\'erin}
\IEEEauthorblockA{LIPN, University of Paris 13\\
Paris, France\\
christophe.cerin@lipn.univ-paris13.fr}

}

%\author{\IEEEauthorblockN{Yanik Ngoko}
%\IEEEauthorblockA{Qarnot Computing and University of Paris 13\\
%Paris, France\\
%yanik.ngoko@\{qarnot-computing.com, lipn.univ-paris13.fr\}}
%\and
%\IEEEauthorblockN{Christophe C\'erin}
%\IEEEauthorblockA{University of Paris 13\\
%Paris, France\\
%christophe.cerin@lipn.univ-paris13.fr}
%}


% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}

Traditional automatic tuning systems are based on an exploration-exploitation tradeoff that consists of: 
learning the behavior of the algorithm to tune on several benchmark (exploration) and then using the learned  
behavior for solving new problem instances.
On NP-hard  algorithms, this vision is criticizable because of:  the hardness of finding a reference 
benchmark and  the potential huge runtime of the exploration phase.
In this paper, we introduce QTuning, a new automatic tuning  system specially designed for NP-hard 
algorithms. Like traditional tuning systems, QTuning is based on benchmark runs. But, during the learning process, new benchmark 
data can always be introduced or existing ones removed. Moreover, the system mixes the exploitation and exploitation phases.  
The main contribution of this paper is to formulate the learning process in QTuning within an active 
learning framework. The framework is based on a classical observation made 
in optimization: namely, the efficiency of random search in regret minimization.  We improve our random search 
algorithm in including a machine learning classification approach and a set intersection problem. 
Finally, we discuss about the experimental evaluation of the framework for the resolution of the satisfiability problem.


\end{abstract}

\begin{IEEEkeywords}
Automatic tuning; Random search; Active learning;  Maximum Subset Intersection Problem.

\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}

The global objective of this study is to improve the performance of cloud-services in learning their optimal 
configuration in an  in-situ context. We assume a  cloud-service  made of configurable 
time-consuming algorithms like exponential algorithms or  NP-hard algorithms. We also 
assume that the configurable algorithmic parameters are well identified and that benchmarks for their calibration exist. 
In strickly using the cloud resources, our objective is to build an automatic tuning system that will find    
the configurations that optimize the cloud-service on runtime and/or energy consumption.

The core problem that we address is not new. It was addressed in several past work with  
dense and sparse linear algebra algorithms~\cite{ATLAS,Spiral}, sorting~\cite{Spiral} or Fast Fourier Transform~\cite{FFTW} 
algorithms. Fundamental principles and techniques for automatic tuning have also been formulated in methods, 
systems and frameworks~\cite{Rice,Tapus:2002:AHT:762761.762771,ansel:cases:2012}. Despite these contributions, our work bring 
several novelties when considering the state-of-art of automatic tuning techniques. 

Traditional automatic tuning systems are of two kinds: {\it offline and runtime tuning}~\cite{Tichy:2014:APS:2636925.2636340}. 
In both classes, the tuning is built upon an exploration-exploitation tradeoff where from a parametrizable algorithm $A(.)$ 
and a reference benchmark $B$ (set of problem instances), one {\it learns} the optimal parameters values to use in $A(.)$ 
such as to  minimize the runtime or energy consumption induced by its processing. 
After this exploration phase, the optimal parameter values are used for solving other input instances of $A(.)$ (exploitation phase). 
In the case of offline tuning, the exploration includes several runs of $A(.)$ on entries of $B$ (for performance profiling) 
and produces an optimal configuration $\vec{\theta}^{opt}$ that is used for runing $A(.)$. 
In runtime tuning, instead of an optimal configuration, the exploration generates a {\it decision function} $f$ 
that on any input $I$ of $A(.)$ returns the optimal configuration ($\vec{\theta}^{opt} = f(I)$) to use in its processing. 
Offline and runtime tunings have been succesfully applied to many computational problems. However, we do believe that 
these approaches are not suitable to exponential and NP-complete problems. 

Our first disagreement comes from the fact that this exploration-exploitation tradeoff {\it requires a reference or representative 
benchmark} that characterizes the {\it key features} of the inputs accepted by  $A(.)$. Unfortunately, on NP-hard problems, the great 
diversity of problem instances makes it hard to reach a consensus on reference benchmark. International competitions like the annual SAT 
competition~\footnote{http://www.satcompetition.org/} or DIMACS challenge tend to establish such references; but, one can notice 
that their reference benchmarks change every year. 
Our second disagreement comes from the fact that these two tuning techniques assume a profiling stage where performance of 
algorithms are estimated on a reference benchmark. On exponential or NP-hard problem, this stage could take several days, 
months or years. 


In the light of these observations, we argue that the first difference between our work and prior ones is to think 
automatic tuning in an alternative exploration-exploitation tradeoff, more suitable for NP-hard and exponential problems. 
We formulate our alternative approach as the {\it active learning perspective for automatic tuning}. 
In this vision, the tuning is not made of two separated phases of exploration and exploitation. Both phases co-exist 
such that the tuned system evolve throughout the time with new optimal or sub-optimal configurations, discovered in the processing. 
In addition, we reject the idea of a reference benchmark and prioritize the one of dynamic training sets that users 
can configure and modify, on the fly, during the tuning. 
%Let us observe that there exists work that studied active learning for automatic tuning~\cite{DBLP:conf/cluster/BalaprakashGW13}. 
%We differ from them on the type of algorithms we intend to tune and on the exploration and exploitation algorithms we propose.

The second difference between our work and existing ones is that we consider functioning cloud-services to tune in-situ. 
This means that the datacenter resources on which we operate is not dedicated to tuning. Therefore, the exploration process will  
evolves in an agile setting where resources are not always available or execution faults could occur. 

Summarizing, in this paper, we introduce QTuning, a new automatic tuning service that implements the active 
learning perspective for tuning, in-situ, in a cloud context. The paper contribution focuses on the general 
architecture of the system and its active learning framework. On this last point, we formulate the challenge in 
exploration as a bi-dimensional random search problem that we address in solving a classification problem and 
an NP-hard set intersection problem.  Finally, we provide an experimental evaluation of our framework.

The remainder of the paper is organized as follows. In Section~\ref{Motivation}, we explain the practical motivation of 
our work. In Section~\ref{Related}, we discuss the related work. 
In Section~\ref{Model}, we introduce the key automatic tuning concepts that are manipulated in QTuning. 
The architecture of the system and its active learning framework is presented in Section~\ref{Architecture}. 
In Section~\ref{Exploration}, we discuss about optimization algorithms implemented in the framework. 
A practical validation is presented in Section~\ref{Proof-of-concept} and we conclude in 
Section~\ref{Conclusion}.


\section{Motivation} \label{Motivation}

Our initial motivation for designing QTuning was to improve the {\it qombinatorics} 
cloud service, available in the Qarnot Computing cloud~\footnote{http://www.qarnot-computing.com/}. Qombinatorics is a SaaS 
developed by the Qarnot Computing Research team for the parallel resolution of NP-hard problems.

The system is deployed in the Qarnot Computing cloud: a heating cloud for HPC that inovated in designing special radiators that produce heat from computations. The Qarnot cloud infrastructure consists of the network of radiators that are deployed in homes and are managed by a private resource manager (Qware). 
In an economic viewpoint, the Qarnot business is based on two types of users: hosts interested in heating and HPC users interested 
in computing. The objective is to use requirements in computations for supplying those in heating.
However, it is obvious to notice that such a system is not always balanced. Typically, in winter, it can happen that we 
do not have enough computations for heating. For these situations, the QTuning system is particularly interesting. 
The idea is to push profiling jobs for learning the behavior of services implemented in the cloud when radiators are idle. 
The conclusion of the learning are next used to improve the functioning of the cloud-services on user requests.




\section{Related work} \label{Related}

As stated in the introduction, automatic tuning  is a well-investigated field. For a general survey on HPC systems based on 
automatic tuning, the interested reader might take a look at the general report of the 2014 Dagstuhl seminary~\cite{benkner_et_al:DR:2014:4423}. 
It is also important to notice that nowadays, automatic tuning, as well as  {\it parallelism}, more than in the past 
are considered by industrials as one of the major strategy for the improvement of algorithmic performance. The invest on 
performance tuning of SQL requests made by major companies  like Oracle or IBM is an example~\cite{Oracle}. 

For positioning our study, we propose to classify automatic tuning systems based on two criteria: 
their generecity and the characteristic of their learning process. On the first criteria, there are two classes to consider: 
{\it problem-specific} and {\it class-of-problems} ones. On the second criteria, we will consider 
{\it passive learning} and {\it active  learning} approaches.

\subsection{Problem-specific vs class-of-problems tuning}

By problem-specific, we refer to an automatic tuning approach that was developed for specific problems. 
In the class of problem specific classes class, some convincing examples are the ATLAS~\cite{ATLAS} library for matrix multiplication, 
OSKI~\cite{Vuduc:2005zi} on sparse matrix vector multiplication,  the SPIRAL library~\cite{Spiral} for sorting and the FFTW~\cite{FFTW}  
for Fast Fourrier transform. These work demonstrated the efficiency of some key automatic tuning techniques as the offline and  
 runtime tuning or the dynamic programming approach for algorithms cascading. One general trend in these work is that 
the tuning targets the runtime optimization by finding between different versions of a program, the one that is optimized on data 
access. At this stage, we highlight that in comparison to these prior works, the tuning system that we propose is not restricted to 
a specific problem or type of parameters. Instead, we consider general 
techniques that can be used in a wide range of NP-complete and exponential algorithms.

There are several sytems or frameworks that were proposed for automatic tuning. The AEOS approach~\cite{AEOS} 
summarizes a set of general principles and a component architecture that were succesfully applied for tuning  
serveral dense linear algebra kernel. As discussed before, we do not believe that this solution is suitable for the context we target. 
The OpenTuner framework formulates a new language for general automatic tuning~\cite{DBLP:conf/IEEEpact/AnselKVRBOA14}. 
The system also innovates in implementing ensemble techniques for combining several automatic search algorithms. As OpenTuner, QTuning 
invests on search techniques for tuning exploration. However, we propose to use alternative search techniques. 
 The Periscope framework~\cite{DBLP:conf/parco/MijakovicSUGSC13} is another general solution for automatic 
tuning in a distributed context. This solution is particularly interesting since it also addresses the tuning for energy efficiency. 
 As Periscope, QTuning is based on distributed search, but our objective in distribution is to diversify the quality of local solutions 
that can be found during the search. Moreover, QTuning differs on how it makes online synthesis of the best tuning results.
ParamILS~\cite{Hutter:2009:PAA:1734953.1734959} is an automatic tuning system especially designed for tuning hard computational 
algorithms like SAT. The exploration algorithms used in QTuning are inspired from the random search algorithm defined in 
ParamILS. But, we innovate in formulating this search as a combination between breadth and depth first searches for optimizing 
a regret function. In addition, to the contribution of ParamILS, we introduce a new modeling of the data exploitation challenge. 

***!!! problem-specific can in some cases be generalized ******
\subsection{Passive learning vs active learning}


As mentioned in the introduction, there already exist work that envisionned the automatic tuning as a long-term process. In particular, there is the MATE system~\cite{DBLP:conf/para/MorajkoMCMS10}, the tuned plugin~\footnote{http://linux.die.net/man/8/tuned} available 
in Linux Redhat or the Green framework~\cite{Baek:2010:GFS:1809028.1806620}. However, none of these work formulated the 
automatic tuning problem as us; while they invested on the idea of improving a system by monitoring, we consider automatic 
tuning as an online search problem.



**** MIX WITH ACTIVE LEARNING STATE of ART*** E.G: Let us observe that there exists work that studied active learning for automatic tuning~\cite{DBLP:conf/cluster/BalaprakashGW13}. 
We differ from them on the type of algorithms we intend to tune and on the exploration and exploitation algorithms we propose


Finally, let us recall that one of the earlier general automatic tuning systems is the  Algorithm selection 
framework of John Rice~\cite{Rice}. This work innovated in introducing the key concepts that are nowadays used in most automatic 
tuning system. Our work is largely inspired by the Rice framework.

In the next section, we will now introduce the key modeling concepts that are manipulated in QTuning. The objective is to 
give more details about the active learning vision implemented in QTuning. 


\section{Model} \label{Model}

In this part, we formally describe the automatic tuning problem addressed in QTuning. We first describe 
the general concepts and then the problem formulation according to the active learning framework.
Â \subsection{Basic concepts}

In QTuning, automatic tuning problems are formulated within the notion of tuning project.  Such  
a project can be  constrained or unconstrained. We explain these notions below.

\begin{definition}{\bf (Unconstrained tuning project)}
We define an unconstrained tuning project as a tuple $ \Gamma = (\sigma, B, \vec{\theta}, d(\vec{\theta}))$. Here, $\sigma$ is the cloud service to tune; 
$\vec{\theta}$ are the parameters to tune and $d(\vec{\theta})$ is the set of definition domains for each parameter 
$\theta_i \in \vec{\theta}$; $B$ is the initial set of training data that will be used for the tuning.
\end{definition}

An unconstrainted tuning project defines a corpus of data to use for learning the behavior of a cloud-service. 
Once such a project is submitted, the system will start a learning process for finding the {\it best parameters} for 
running $\sigma$. Any submitted project can be modified, on the fly during the learning process. Such a modification can consist of  
the addition of a new entry in $B$, the deletion of an entry and the deletion of a parameter from $\vec{\theta}$. 

Let us assume that $\vec{\theta} = (\theta_1,\dots \theta_n)$. In QTuning, an unconstrained tuning project defines 
a tuning problem whose solution is a {\it configuration} $\bar{\theta}_u \in D$ where $D = d(\theta_1) \times \dots  \times d(\theta_n)$. 
In order to be suitable to a large class of cloud-service, QTuning makes few assumptions on the structure of $D$. 
Consequently, the search space can be extremely large. 
In some cases however, users might want to introduce {\it particular knowledge} that could reduce the size of the search space. 
For this purpose, QTuning supports the concept of {\it tuning constraint} defined as follows.

\begin{definition}{\bf (Tuning search space constraint)}
We define a constraint of the search space of a tuning project as a tuple $C = ((\theta_i, v_i), (\theta_j, v_j),\dots (\theta_k, v_k))$. 
The constraint states that we do not authorize the assignments in which $\theta_i$ is assigned to $v_i$, $\theta_j$ to $v_j$ etc.
\end{definition}


Constraints can be defined in any tuning project. This leads to a constrained tuning project that we define 
 as a tuple $ \Gamma = (\sigma, B, \vec{\theta}, d(\vec{\theta}), C)$ where $C$ is the set of constraints of the project. 
In the following, we will only consider constrained tuning project because they generalize unconstrained ones. 


For caracterizing the solution of a tuning project, let us assume that on each benchmark entry $\beta_i \in B$ 
and assignment of parameters $\bar{p}_u$ we have an estimation $\Delta(\sigma, \bar{p}_u, \beta_i)$ of the 
average runtime required for processing $\beta_i$ by $\sigma$. Then we have the following definition.

\begin{definition}{\bf (Solution of a tuning project)}
The solution of a tuning project $ \Gamma = (\sigma, B, \vec{\theta}, d(\vec{\theta}), C)$  is the configuration 
$\bar{\theta}^o$ such that \[ \bar{\theta}^o = \arg \underset{\{\bar{\theta} | C \text{ are not violated }\}}{\min} \frac{1}{|B|}\sum_{\beta_i \in B} \Delta(\sigma, \bar{\theta}, \beta_i) \] 
\end{definition}

In other words, the solution of a tuning project is the vector of parameters that leads to the best average 
cost in the processing of the benchmark instances. This definition restricts our objective to the tuning for average runtime 
optimization. But, the conclusion of our work can be generalized to alternatives optimization like makespan or energy 
consumption minimization.  

The basic concepts we introduced formalize the automatic tuning problem as encountered in several studies. 
QTuning extend these traditional notions in assuming a particular formulation of the tuning problem. 

\subsection{The min-regret tuning problem}

Once a project $ \Gamma = (\sigma, B, \vec{\theta}, d(\vec{\theta}), C)$ is submitted in QTuning at an initial 
date $t_0$, the cloud-service $\sigma$ can at anytime $t$ request the best local configuration that the 
system produced. This configuration will be used for processing $\sigma$ in parallel to the  learning stage. 
Moreover, the project submitted at the date $t_0$ could be changed at any date according to the actions we define 
in the prior section. Therefore, in searching for the best configuration, the QTuning system must try at anytime 
to return a solution that is not too far from the optimal one, according to the knwoledge it has on tuning projects. 

Given a project $\Gamma$, let us assume that at the date $t$, its setting is defined as $\Gamma(t)$. 
Let us also assume that the optimal solution of $\Gamma(t)$ is $\bar{\theta}^o(t)$. 
In the min-regret tuning problem, we are interested in minimizing the regret, defined as the 
difference between the average cost for processing $B(t)$ with $\bar{\theta}(t)$ and the one cost for 
processing $B(t)$ with the best possible configuration. We formalize the regret at the date $t_n$ as: 

\[
\int_{t_0}^{t_n} \left( Cost(\bar{\theta}(t)| \Gamma(t)) - Cost(\bar{\theta}^o(t) | \Gamma(t)) \right) dt
\]
where, 
\[
Cost(\bar{\theta}(t) | \Gamma(t)) = \frac{1}{|B(t)|}\sum_{\beta_i \in B(t)} \Delta(\sigma, \bar{\theta}, \beta_i) 
\]


In comparison to classical automatic tuning problems, the min-regret problem brings two novelties: 
the notion of evolving tuning projects and the regret minimization. 
We end here the modeling of the automatic tuning problem. In the next section, we will describe how 
QTuning addresses the min-regret problem in a system and algorithmic viewpoint.

\section{The Qtuning architecture} \label{Architecture}

\subsection{General view}

	\begin{figure*}[hbtp]
	\begin{center}
	%\fbox{
	\input{./Figures/QTuningArch.tex}
	%}
	\caption{In-situ computing with QTuning}
	\label{fig:QTuning}
	\end{center}
	\end{figure*}


In Figure~\ref{fig:QTuning}, we describe the typical deployment architecture for which the 
QTuning system have been built. QTuning is composed of an API ({\it QTuning.dll}) and 
a server program ({\it QTuning.d}). The QTuning API includes a set of functions that 
launch remote executions on the servers. The functions can request: the loading of a new tuning project (submission of 
$\Gamma(t)$), the modification of an existing one or the downloading of analytics results about a tuning project 
(typically, the downloading of $\theta(t)$). The interest in implementing these functions in an API is 
to allow any cloud-service to adapt its behaviour on the fly. In Figure~\ref{fig:QTuning} for instance,  
the cloud {\it service.d} could decide to request the best local configuration before processing any user request. 
It can also make these requests according to a specific statistic law.

Let us now assume that {\it service.d} pushed a tuning project $\Gamma$ in  {\it QTuning.d}. Then, 
{\it QTuning.d} will start its  learning process for finding the configurations that minimize the regret. 
For this purpose, {\it QTuning.d} will frequently create {\it exploration jobs} whose objectives are to run 
 {\it service.d} on distinct configurations. Created jobs are submitted to the cloud resource manager (RMS in 
Figure~\ref{fig:QTuning}) for a processing on cloud resources (In the case of Qarnot, they are Q.rads). 
Performance results about these jobs are next collected and save in a Mongodb database~\footnote{www.mongodb.org}. 
They will be used for computing the best local configurations that was found in the learning process.

In this architecture, we chose the Mongodb database manager because it is adapted to the storage of massive data. 
This is expected in our active learning perspective where the benchmark can be extremly large.
One can also remark that the deployment of QTuning requires that it supports the cloud resource manager 
interface (for jobs submission). In its current version, the service was developped for the Qware 
system (of Qarnot Computing); but we envison to include the API of other resource managers. 
In what follows, we will now give a deeper presentation of the QTuning learning framework.



\subsection{QTuning active learning framework} 

The QTuning addresses the min-regret tuning problem with an {\it active learning framework} 
based on four concepts: data exploration algorithms, data exploitation algorithms, 
exploration notifications and exploitation notifications. These concepts fit together as follows. Let us consider 
a tuning project  $\Gamma = (\sigma, B, \vec{\theta}, d(\vec{\theta}), C)$. The submission of such a project 
in the QTuning system will automatically cause an exploration notification. This notification implies 
to start a data exploration algorithm. The goal of a data exploration algorithm is to  
compute the costs $\Delta(\sigma, \bar{\theta}, \beta_i)$  induced by the run of  $\sigma(\bar{\theta})$ on the benchmark 
entry $\beta_i$.  At a moment in the system, we could then have a set of evaluations 
$\Delta(\sigma, \bar{\theta_1}, \beta_0), \dots \Delta(\sigma, \bar{\theta_k}, \beta_n)$. If the service $\sigma$ 
 requests for the best local configuration, this will cause an exploitation notification. Such a notification will 
start a data exploitation algorithms that will return the best local configuration that could be derived from 
$\Delta(\sigma, \bar{\theta_1}, \beta_0), \dots \Delta(\sigma, \bar{\theta_k}, \beta_n)$.

In the above scenario, we presented exploration and exploitation notifications as being caused by a project submission or a cloud-service 
request. In QTuning, there are other situations where these notifications are launched; but, this is not discussed in this paper.
Data exploration algorithms are naturally embarassingly parallel. It suffices to concurrently evaluate 
the $\Delta(\sigma, \bar{\theta_u}, \beta_i)$. The challenging question that this observation leads to is the 
one of deciding on the resource set to assign to  exploration. Indeed, the more we have resources for exploration 
the more we can expect to minimize the regret. Nonetheless, we must not forget that $\sigma$ must also service 
user requests and for this, will need computing resources. In its current implementation, QTuning only uses 
idle resources for exploration. In addition, the exploration could be interrupted at anytime if a user 
request is to be processed. Finally, a subset of resource is permanently allocated for data exploitation algorithms. 

In Algorithm~\ref{alg0}, we summarize the functioning of the active learning framework.

	\begin{algorithm}                    
	\caption{\scriptsize Active Learning framework} 	\label{alg0}  
	\begin{algorithmic}[1]
	\scriptsize
        \WHILE{ true}
	\STATE {\bf ON\_EXPLORATION\_NOTIFICATION (e):}
        \WHILE{ e.exploration\_continue()}
        \STATE get the maximal number $max_J$ of resources available for exploration
        \STATE get the maximal number $cur_J$ of resources used by exploration jobs
        \FORALLP{ $j$ in $\{1,\dots,max_J-cur_J\}$ }
	\STATE Query an Oracle $O$ on the next configuration point $(\bar{\theta}, B)$ to evaluate
	\STATE Submit an exploration job for $(\bar{\theta}, \beta)$ to the RMS
        \STATE Wait for result 
        \STATE Process and save the performance $\Delta(\sigma, \bar{\theta}, \beta)$ in the database
        \ENDFOR
	\ENDWHILE
	\STATE {\bf ON\_EXPLOITATION\_NOTIFICATION (e):}
        \STATE Retrieve the identifier $pid$ of the corresponding tuning project from $e$
	\STATE Load the performance data $\Delta(\sigma, \bar{\theta_1}, \beta_0), \dots 
\Delta(\sigma, \bar{\theta_k}, \beta_n)$ of $pid$ from the database
	\STATE Process the analytic request formulated in $e$.
        \STATE return the result
	\ENDWHILE
	\end{algorithmic}
	\end{algorithm}
	\normalsize

Algorithm~\ref{alg0} gives a simplified view of what is actually implemented. For instance,  
in the exploration phase, it does not handle the fact that QTuning actually manages several distinct tuning projects.
Another limitation is that the execution of an exploration job could fail and this is supported in QTuning. 
However the presented view is enough for understanding the main principle of data exploration. 
On data exploitation, let us notice that until now, we mainly focused on a vision where exploitation 
consisted in retrieving the best local configuration. However, QTuning has a larger view. The idea is to process 
analytic requests that can consist in computing the standard deviation observed in a configuration or the 
percentage of the search space that was explored on a configuration. As for exploration, the exploitation 
scheme gives a simplified view of what is implemented. For example, QTuning generally anticipates requests 
on analytic results in computing them before (at line 10). 

It might not be clear why we used the term active learning when considering the literature on the concept. 
Indeed, we can relate our framework to active learning algorithms studied in 
classification as follows. The {\it configuration points} ($(\bar{\theta}, \beta)$) for 
which  $\Delta(\sigma, \bar{\theta}, \beta)$ are our labelled data. The points without such an estimation 
are our unlabelled data. The best configuration is the classifier we are building. The objective  
in querying the oracle $O$ is to state the {\it most important configuration point} that could serve for finding a 
configuration, close to the best one. Finally, as other active learning systems~\cite{Chu:2011:UOA:2020408.2020444}, 
we are motivated by the potential huge training time (estimation of $\Delta(\sigma, \bar{\theta}, \beta)$) and  
data to process. 

We end here the general presentation of the QTuning framework. In the next sections, we will 
discuss in more details about data exploration and exploitation algorithms.



\section{Data exploration and exploitation in QTuning} \label{Exploration}

This section aims at developing two points of the prior framework. The first is the 
different type of oracles that we can use in the exploration; the second one is how we compute the 
 the best configuration depending on the chosen  oracle. 

\subsection{The lexicographic  oracle (Lex)}

The objective of QTuning oracles is to return the most important configuration point to evaluate.
The lexicographic  oracle works by ordering the configurations and benchmark according to the lexicographic order. 
The order is first applied on configurations and then on benchmark. For any query, it returns the next pair 
($(\bar{\theta}, \beta)$) in the lexicographic order that was not already evaluated. 

For the lexicographic  oracle, the computation of the best configuration (line 10 in the active learning framework) 
is simple: we cumulate the estimation made on a configurations and compare with the best result already found. 
This process is detailed in Algorithm~\ref{alg1}. 

	\begin{algorithm}                    
	\caption{\scriptsize Lexicograpic Result Update } 	\label{alg1}  
	\begin{algorithmic}[1]
	\scriptsize
        \STATE {\bf INPUT:} A new result  $\Delta(\sigma, \bar{\theta_i}, \beta_j)$, 
         $\theta^o$ is the best local configuration and $\Delta(\theta^o)$ is its runtime on $B$.
        \STATE Add $\Delta(\sigma, \bar{\theta_i}, \beta_j)$ to $\Delta(\theta_i)$
	\IF{$\Delta(\theta_i) > \Delta(\theta^o)$ }
        \STATE Mark all the pairs $(\theta_i, \beta_u)$ as explored
        \ELSE
      	\IF{all the pairs  ($(\theta_i, \beta_u)$) where evaluated for $\beta_u \in B$}
        \STATE Compare $\Delta(\theta^o)$ and $\Delta(\theta_i)$ and update the best local configuration
        \ENDIF
	\ENDIF
	\end{algorithmic}
	\end{algorithm}
	\normalsize

As one can remark, the active learning framework with the lexicographic order leads to lexicographic 
exploration of the search space with the usage of a lower bound. This leads to a classical search approach that however 
is not optimized for the context we target. Indeed, for the minimization of the regret function, we need a  oracle  
that takes care about the {\it informativeness} of the next configuration point: we do not have 
any element that suggests that the next lexicographic could be such a point. 

\subsection{Random  oracle (Rand)}

Given a tuning project $\Gamma = (\sigma, B, \vec{\theta}, d(\vec{\theta}), C)$, whose search space is $D$, 
 the lexicographic  oracle implicitely assumes that a lexicographic exploration could serve to quickly discover 
the best configuration. But, this assumption is weak. Good configurations could be located {\it anywhere} in 
$D$. It is the {\it potential random distribution} of such points that the {\it Rand}  oracle exploits. 
 
\subsubsection{ Random configuration choice and depth first search.} 
As the lexicographic  oracle, the random  oracle is based on ordering. In particular, it completely processes  
points of a configuration $\theta_{i}$ before starting those of a configuration $\theta_{i'}$.  
Differently from the lexicographic  oracle, when for the current configuration $\theta_i$ 
 we have all the $\Delta(\sigma, \bar{\theta_i}, \beta_j)$s, the next configuration $\bar{\theta_{i'}}$ from 
which we start is chosen randomly. In Figure~\ref{fig:Search}, we graphically represent  how this feature 
makes the random  oracle different from the lexicographic one. With random iterations over configurations, 
we diversify the search walk. For regret minimization, we are convinced that this is better in the mean case. 
Let us now assume that the random  oracle selected a  $\theta_i$ for the generation of the next configuration 
points. A challenging question is to decide on the ordering of the first benchmark entries to evaluate. 
For example, must we start by the point $(\theta_i, \beta_0)$ or by the point $(\theta_i, \beta_1)$?

\subsubsection{ Hard and soft instances.} 

For deciding on the first benchmark entry to evaluate, we use an observation made on NP-hard problems. 
The idea is that instances of NP-hard problems are often classified as being hard or soft. The soft instances are the 
one for which we can quickly find a good solution while hard instances are the ones for which algorithms runtime explode.
Given a new configuration $\theta_i$, we do believe that it will be better to start to evaluate firstly its 
hard instances. Indeed, if $B_{hard}$  are those instances and $\theta_i$ a {\it weak} configuration, 
we can quickly fall in a setting where \[ \sum_{\beta \in B_{hard}} \Delta(\sigma, \bar{\theta_i}, \beta) > \Delta(\theta^o)\]
This means that in using the process of results update described in Algorihm~\ref{alg1}, we can quickly eliminate 
non-optimal configurations if we know hard instances. 

Thus, given a configuration $\theta_i$, we propose in the random  oracle to first classify instances in hard and 
soft sets $B_{hard}$ and $B_{soft}$. Then, we choose  to return configuration points with $B_{hard}$ before 
those in $B_{soft}$. Within each class, the benchmark entries are selected randomly.  
The only remaining question in this scheme  is to be able to effectively build $B_{hard}$ and $B_{soft}$. 
This is discussed in the next.


	\begin{figure}[hbtp]
	\begin{center}
	%\fbox{
	\input{./Figures/Search.tex}
	%}
	\caption{Lexicographic, random and random oracle with wisdom.
        {\it Lex} explores the search space in the lexicographic order. {\it Rand}  and {\it RandW} 
        randomly choose the next configuration. {\it RandW} differs from {\it Rand} on the fact that 
        it extracts information from configurations that were partially explored.}
	\label{fig:Search}
	\end{center}
	\end{figure}


\subsubsection{A similarity approach for finding hard instances.}  

The main assumption we made for detecting hard instances is that their {\it runtime will look similar on the configurations 
we processed}.
Let us assume at the date $t$ that we processed the set of configurations $\Theta(t)$. Let us also assume that 
there is a subset $\Theta_1(t) \subseteq \Theta(t)$ for which the evaluations were performed on all $B$~\footnote{due 
to lower bound elimination we could stricly have $\Theta(t) \setminus \Theta_1(t) \neq \emptyset$}. 
We propose to characterize the runtime of each instance $\beta \in B$ as a vector \[ \Delta(\beta) = [\Delta(\sigma, \bar{\theta^{'}_0}, \beta), \dots \Delta(\sigma, \bar{\theta^{'}_{n_1}}, \beta) ]^T \] where $n_1 = |\Theta_1(t)|$. Here, $\theta^{'}_{i}$ are configurations in $\Theta_1(t)$ 
that we ordered randomly.

In other words, the runtime vector of a benchmark entry is the collection of runtimes we observed on $\Theta_1(t)$. 
Given these data, we propose to apply a $2-$Means clustering algorithm on the data $B$ with the runtime 
vector and the euclidian distance. Finally, we conclude that hard instances are those, in the class, for 
which the mean value $\norm{\Delta(\beta)}$ is the greatest. Indeed, it is normal to expect that on hard instances, 
most configurations take a huge runtime before finding the optimal solution. 

Let us mention that the idea of distinguishing hard instances of NP-hard problem with a machine learning approach was also 
investigated in another paper~\cite{WZZReport}. But our approach completely differs from this proposition on the modeling 
of features. 
The {\it Rand} oracle uses the result update process of {\it Lex}. However, it is based on a deeper exploitation 
of the potential random distribution of best configurations. In addition, it exploits more past information generated in 
the exploration to take further decisions. But, this point could be improved as we will see in the sequel.



\subsection{Random  oracle with the wisdom of the loosers (RandW)}

With {\it Rand}, estimations made on  $\bar{\theta^{'}_0}$ could serve 
to guide the exploration of $\bar{\theta^{'}_{1}}$, $\bar{\theta^{'}_{2}}$ etc. This extraction of information is 
interesting, but there still remains a wastage. Indeed, data related to the {\it losing configurations} 
$\Theta_2(t) = \Theta(t) \setminus \Theta_1(t)$ are not exploited by the oracle~\footnote{Losing configurations 
are those on which we did not entirely evaluated $B$ because we detected before a lower bound violation.}. 
This can be seen as a loss if we consider the processing cost that these evaluations required. The question then 
is: how could we exploit {\it this effort} for taking better decision? 

For this purpose, let us come back to the random  oracle. At the begining of the exploration of any 
configuration $\theta_i$, the algorithm separates instances in hard and softs. Then, it randomly chooses  
instances in the hard class and then the ones of the soft class. This random choice could be improved 
in considering data of losing configurations. For example, beeing given the set hard instance $B_{hard}$,  
we could observe from data of the loosers that a small subset $B_{hard^+}$ of these data was generally 
enough to reach a lower bound violation. Then, instead of a random exploration of $B_{hard}$, 
it will be more efficient to randomly start by instances in $B_{hard^+}$. 
Globally, we can then reformulate the exploration of a configuration points as follows. We first start with the benchmark 
entries in  $B_{hard^+} \cap B_{hard}$, then by the one in $B_{hard^+} \cap B_{soft}$ and finally the remaining 
instances in considering hard instances before soft ones. The question is: how to build $B_{hard^+}$? 


\subsubsection{Computation of hard instances as a subset intersection problem.} 

We propose to consider $B_{hard^+}$  as the set of benchmark entries for which there is 
a maximal number of losing configurations that were evaluated. The idea behind is that 
the more a same benchmark entry was evaluated in losing configurations, the more, we 
could believe that this benchmark entry causes lower bound violations. 

More practically, at each date $t$, we assume that we know the size $K(t)$ of $B_{hard^+}$ (this will be discussed latter). 
For computing $B_{hard^+}$, we firstly associate each benchmark entry $\beta$ with a 
configuration set $S(\beta)$ whose elements are all configurations  $\bar{\theta} \in \Theta_2(t)$ for which 
there exists a computed estimation $\Delta(\sigma, \bar{\theta}, \beta)$. We then choose 
$B_{hard^+}$ as the set of benchmark entries $\{ \beta^+_1, \dots, \beta^+_k \}$ such that 
$|\{ \beta^+_1, \dots, \beta^+_k \}| = K(t)$ and 
 $|S(\beta^+_1) \cap \dots \cap S(\beta^+_k)|$ is maximal. 


With this formulation, we will decide on the construction of $B_{hard^+}$ based on 
 $|S(\beta^+_1) \cap \dots \cap S(\beta^+_k)|.K(t)$  additional data. In the best case, 
when comparing with {\it Rand}, we will have  $|\Theta_2(t)|.K(t)$ additional data that {\it we will transform in information}.
This formulation has two challenges. The first one is that we need to tune $K(t)$. In our implementation, we propose 
to randomly chose these values. The second challenge is the resolution of the intersection problem we introduced. 


\subsubsection{ Greedy algorithm for computing $B_{hard^+}$.}

One can easily notice that the computation of $B_{hard^+}$ as formulated correspond to the 
resolution of the  Maximum K-intersection problem~\cite{DBLP:journals/ipl/ShiehTY12}. 
Indeed, this latter problem is defined as follows.

Given $n$ sets $E_1,\dots,E_m$ with elements over a universe $\Sigma = \{ e_1,\dots,e_n Â \}$, the goal is to select 
$K$ subsets of $E_1,\dots,E_m$ in order to maximize the size of the intersection of the sets.
As shown in~\cite{DBLP:journals/ipl/ShiehTY12}, this problem is NP-complete and cannot be approximated within 
an absolute error of $\frac{1}{2}n^{1-2\epsilon} + \frac{3}{8}n^{1-3\epsilon} - 1$ unless {\bf P = NP}.
However this intersection problem is close to coverage problems for which there exists  
a general heuristic framework. The first stage in this framework consists of ordering the sets $S(\beta)$ according 
to a pricing strategy. Then, the sets with higher prices are progressively chosen until we reach a subset intersection 
that cannot evolve. Using this idea, we propose in Algorithm~\ref{alg4} a greedy solution for building the 
$B_{hard^+}$ set.

	\begin{algorithm}                    
	\caption{\scriptsize RandW Hard instance selection (RandW-HIS)} 	\label{alg4}  
	\begin{algorithmic}[1]
	\scriptsize
	\FOR{$\beta \in \Theta_2(t)$}
	\STATE Build the set $S(\beta)$.
	\ENDFOR
	\STATE $C = \emptyset$; $Inter = \Theta_2(t)$
	\WHILE{$|C| < K(t)$}
		\STATE Choose the set $S(\beta)$ s.t. $S(\beta)$ is not in $C$ and $|S(\beta) \cap Inter|$ is maximal;
		\STATE $C  = C \cup S(\beta)$
                \STATE $Inter = Inter \cap S(\beta)$
	\ENDWHILE
	\STATE return $C$
	\end{algorithmic}
	\end{algorithm}
	\normalsize



We end here the description of the data exploration and exploitation in QTuning. In the next, we will then present an 
experimental evaluation of our different approaches.



\section{Experimental evaluation} \label{Proof-of-concept}

 Evaluate Lex, Rand, RandW, Rand(Class sorting), RandW(with soft-and-hard), try different $K(t)$

\section{Conclusion} \label{Conclusion}


Perspective: 

Combine oracles. Refine classification for filling. Learn $K(t)$

% conference papers do not normally have an appendix


% use section* for acknowledgement
\section*{Acknowledgment}


The authors would like to thank...
more thanks here


\bibliographystyle{./IEEEtran}
\bibliography{self_learning}




% that's all folks
\end{document}

f
